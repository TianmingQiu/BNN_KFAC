{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling-free Laplace Approximation for Bayesian Neural Network\n",
    "This notebook demonstrates how to compute the KFAC approximations of the Fisher information matrix from PyTorch models, as well as how to perform approximate Bayesian inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# choose cuda\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "# standard imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from the repository\n",
    "from models.wrapper import BaseNet\n",
    "from models.curvatures import BlockDiagonal, KFAC, EFB, INF\n",
    "from models.utilities import calibration_curve\n",
    "from models import plot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Basic functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gradient(y, x, grad_outputs=None):\n",
    "    '''\n",
    "    Compute dy/dx @ grad_outputs\n",
    "    y: output\n",
    "    x: parameter\n",
    "    grad_outputs: the “vector” in the Jacobian-vector product\n",
    "    '''\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs = grad_outputs, create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "    return grad\n",
    "\n",
    "def jacobian(y, x, device):\n",
    "    '''\n",
    "    Compute dy/dx = dy/dx @ grad_outputs; \n",
    "    y: output, batch_size \n",
    "    x: parameter\n",
    "    '''\n",
    "    jac = torch.zeros(y.shape[1], torch.flatten(x).shape[0]).to(device)\n",
    "    for i in range(y.shape[1]):\n",
    "        grad_outputs = torch.zeros_like(y)\n",
    "        grad_outputs[:,i] = 1\n",
    "        jac[i,:] = torch.flatten(gradient(y, x, grad_outputs))\n",
    "    return jac\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Load MNIST dataset and split it into two parts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "models_dir = 'theta'\n",
    "results_dir = 'results'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# load and normalize MNIST\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "datasets.MNIST.resources = [\n",
    "    ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "    for url, md5 in datasets.MNIST.resources\n",
    "]\n",
    "\n",
    "train_set = datasets.MNIST(root=\"./data\",\n",
    "                                        train=True,\n",
    "                                        transform=transforms.ToTensor(),\n",
    "                                        download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "\n",
    "# And some for evaluating/testing\n",
    "test_set = datasets.MNIST(root=\"./data\",\n",
    "                                        train=False,\n",
    "                                        transform=transforms.ToTensor(),\n",
    "                                        download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Define the network model and train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net = BaseNet(lr=1e-3, epoch=3, batch_size=32, device=device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "net.train(train_loader, criterion)\n",
    "sgd_predictions, sgd_labels = net.eval(test_loader)\n",
    "print(f\"MAP Accuracy: {100 * np.mean(np.argmax(sgd_predictions.cpu().numpy(), axis=1) == sgd_labels.numpy()):.2f}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 Compute the Kronecker factored FiM "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfac = KFAC(net.model)\n",
    "\n",
    "for images, labels in tqdm(train_loader):\n",
    "    logits = net.model(images.to(device))\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    # A rank-1 Kronecker factored FiM approximation.\n",
    "    labels = dist.sample()\n",
    "    loss = criterion(logits, labels)\n",
    "    net.model.zero_grad()\n",
    "    loss.backward()\n",
    "    kfac.update(batch_size=images.size(0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6 Calculate the inversion of H and Q"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "estimator = kfac\n",
    "add = 1\n",
    "multiply = 200\n",
    "estimator.invert(add, multiply)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7 Evaluate model performance on testset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "targets = torch.Tensor()\n",
    "kfac_prediction = torch.Tensor().to(device)\n",
    "kfac_entropy_lst  = []\n",
    "for images,labels in tqdm(test_loader):\n",
    "    # prediction mean, equals to the MAP output \n",
    "    pred_mean = torch.nn.functional.softmax(net.model(images.to(device)) ,dim=1)        \n",
    "    # compute prediction variance  \n",
    "    pred_std = 0\n",
    "    idx  = np.argmax(pred_mean.cpu().detach().numpy(), axis=1)\n",
    "    grad_outputs = torch.zeros_like(pred_mean)\n",
    "    grad_outputs[:,idx] = 1\n",
    "    for layer in list(estimator.model.modules())[1:]:\n",
    "        g = []\n",
    "        if layer in estimator.state:\n",
    "            if torch.cuda.is_available():\n",
    "                Q_i = estimator.inv_state[layer][0]\n",
    "                H_i = estimator.inv_state[layer][1] \n",
    "                for p in layer.parameters():    \n",
    "                    g.append(torch.flatten(gradient(pred_mean, p, grad_outputs=grad_outputs)))\n",
    "                J_i = torch.cat(g, dim=0).unsqueeze(0) \n",
    "                H = torch.kron(Q_i,H_i)\n",
    "                pred_std += torch.abs(J_i @ H @ J_i.t()).item()\n",
    "    # uncertainty\n",
    "    const = 2*np.e*np.pi \n",
    "    entropy = 0.5 * np.log2(const * pred_std)\n",
    "    kfac_entropy_lst.append(entropy) \n",
    "    kfac_uncertainty = np.array(kfac_entropy_lst)\n",
    "    # ground truth\n",
    "    targets = torch.cat([targets, labels])  \n",
    "    # prediction, mean value of the gaussian distribution\n",
    "    kfac_prediction = torch.cat([kfac_prediction, pred_mean]) \n",
    "print(f\"KFAC Accuracy: {100 * np.mean(np.argmax(kfac_prediction.cpu().detach().numpy(), axis=1) == targets.numpy()):.2f}%\")\n",
    "print(f\"Mean KFAC Entropy:{np.mean(kfac_uncertainty)}%\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8 Evaluate model performance on Gaussian noise images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_entropy_lst = []\n",
    "for i in tqdm(range(10000)):\n",
    "    noise = torch.randn_like(images)\n",
    "    pred_mean = torch.nn.functional.softmax(net.model(noise.to(device)) ,dim=1)        \n",
    "    # compute prediction variance  \n",
    "    pred_std = 0\n",
    "    idx  = np.argmax(pred_mean.cpu().detach().numpy(), axis=1)\n",
    "    grad_outputs = torch.zeros_like(pred_mean)\n",
    "    grad_outputs[:,idx] = 1\n",
    "    for layer in list(estimator.model.modules())[1:]:\n",
    "        g = []\n",
    "        if layer in estimator.state:\n",
    "            Q_i = estimator.inv_state[layer][0]\n",
    "            H_i = estimator.inv_state[layer][1] \n",
    "            for p in layer.parameters():    \n",
    "                g.append(torch.flatten(gradient(pred_mean, p, grad_outputs=grad_outputs)))\n",
    "            J_i = torch.cat(g, dim=0).unsqueeze(0) \n",
    "            H = torch.kron(Q_i,H_i)\n",
    "            pred_std += torch.abs(J_i @ H @ J_i.t()).item()\n",
    "    const = 2*np.e*np.pi \n",
    "    entropy = 0.5 * np.log2(const * pred_std)\n",
    "    res_entropy_lst.append(entropy) \n",
    "    res_uncertainty = np.array(res_entropy_lst)\n",
    "print(f\"Mean Noise Entropy:{np.mean(res_uncertainty)}%\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9 Plot the results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calibration\n",
    "ece_nn = calibration_curve(sgd_predictions.cpu().numpy(), sgd_labels.numpy())[0]\n",
    "ece_bnn = calibration_curve(kfac_prediction.cpu().numpy(), targets.numpy())[0]\n",
    "print(f\"ECE NN: {100 * ece_nn:.2f}%, ECE BNN: {100 * ece_bnn:.2f}%\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(12, 6), tight_layout=True)\n",
    "ax[0].set_title('SGD', fontsize=16)\n",
    "ax[1].set_title('KFAC-Laplace', fontsize=16)\n",
    "plot.reliability_diagram(sgd_predictions.cpu().numpy(), sgd_labels.numpy(), axis=ax[0])\n",
    "plot.reliability_diagram(kfac_prediction.cpu().numpy(), targets.numpy(), axis=ax[1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7), tight_layout=True)\n",
    "c1 = next(ax._get_lines.prop_cycler)['color']\n",
    "c2 = next(ax._get_lines.prop_cycler)['color']\n",
    "plot.calibration(sgd_predictions.cpu().numpy(), sgd_labels.numpy(), color=c1, label=\"SGD\", axis=ax)\n",
    "plot.calibration(kfac_prediction.cpu().numpy(), targets.numpy(), color=c2, label=\"KFAC-Laplace\", axis=ax)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}