{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN KFAC Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 5, 5) # bs x 1 x 28 x 28 -> bs x 5 x 24 x 24\n",
    "        self.pool = nn.MaxPool2d(2, 2) # bs x 5 x 24 x 24 -> bs x 5 x 12 x 12\n",
    "        self.conv2 = nn.Conv2d(5, 10, 5) # bs x 10 x 8 x 8\n",
    "        self.fc1 = nn.Linear(10 * 4 * 4, output_dim) \n",
    "              \n",
    "        self.one = None\n",
    "        self.a2 = None\n",
    "        self.h2 = None\n",
    "        self.a1 = None\n",
    "        self.h1 = None\n",
    "        self.a0 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.one = x.new(x.shape[0], 1).fill_(1)\n",
    "        a0 = x.view(-1, self.input_dim)\n",
    "        self.a0 = torch.cat((a0.data, self.one), dim=1)\n",
    "        \n",
    "        h1 = self.conv1(a0)\n",
    "        self.h1 = h1.data\n",
    "        \n",
    "        a1 = self.pool(F.relu(h1))\n",
    "        self.a1 = torch.cat((a1.data, self.one), dim=1)\n",
    "        \n",
    "        h2 = self.conv2(a1)\n",
    "        self.h2 = h2.data  \n",
    "        \n",
    "        a2 = self.pool(F.relu(h2))\n",
    "        self.a2 = torch.cat((a2.data, self.one), dim=1)\n",
    "        \n",
    "        h3 = self.fc1(a2) \n",
    "        return h3\n",
    "    \n",
    "    def sample_predict(self, x, Nsamples, Qinv1, HHinv1, MAP1, Qinv2, HHinv2, MAP2, Qinv3, HHinv3, MAP3):\n",
    "        # Just copies type from x, initializes new vector\n",
    "        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        for i in range(Nsamples):         \n",
    "            w1, b1 = sample_K_laplace_MN(MAP1, Qinv1, HHinv1)\n",
    "            a = torch.matmul(x, torch.t(w1)) + b1.unsqueeze(0)\n",
    "            a = self.act(a)\n",
    "            \n",
    "            w2, b2 = sample_K_laplace_MN(MAP2, Qinv2, HHinv2)\n",
    "            a = torch.matmul(a, torch.t(w2)) + b2.unsqueeze(0)\n",
    "            a = self.act(a)\n",
    "            \n",
    "            w3, b3 = sample_K_laplace_MN(MAP3, Qinv3, HHinv3)\n",
    "            y = torch.matmul(a, torch.t(w3)) + b3.unsqueeze(0)\n",
    "            predictions[i] = y\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Define KFAC functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def softmax_CE_preact_hessian(last_layer_acts):\n",
    "    side = last_layer_acts.shape[1]\n",
    "    I = torch.eye(side).type(torch.ByteTensor)\n",
    "    # for i != j    H = -ai * aj -- Note that these are activations not pre-activations\n",
    "    Hl = - last_layer_acts.unsqueeze(1) * last_layer_acts.unsqueeze(2)\n",
    "    # for i == j    H = ai * (1 - ai)\n",
    "    Hl[:, I] = last_layer_acts*(1-last_layer_acts)\n",
    "    return Hl\n",
    "\n",
    "def layer_act_hessian_recurse(prev_hessian, prev_weights, layer_pre_acts):   \n",
    "    newside = layer_pre_acts.shape[1]\n",
    "    batch_size = layer_pre_acts.shape[0]\n",
    "    I = torch.eye(newside).type(torch.ByteTensor) # .unsqueeze(0).expand([batch_size, -1, -1])\n",
    "    \n",
    "    B = prev_weights.data.new(batch_size, newside, newside).fill_(0)\n",
    "    B[:, I] = (layer_pre_acts > 0).type(B.type()) # d_act(layer_pre_acts)\n",
    "    D = prev_weights.data.new(batch_size, newside, newside).fill_(0) # is just 0 for a piecewise linear\n",
    "\n",
    "    Hl = torch.bmm(torch.t(prev_weights).unsqueeze(0).expand([batch_size, -1, -1]), prev_hessian)    \n",
    "    Hl = torch.bmm(Hl, prev_weights.unsqueeze(0).expand([batch_size, -1, -1]))\n",
    "    Hl = torch.bmm(B, Hl)\n",
    "    Hl = torch.matmul(Hl, B)\n",
    "    Hl = Hl + D   \n",
    "    return Hl\n",
    "\n",
    "def chol_scale_invert_kron_factor(factor, prior_scale, data_scale, upper=False):\n",
    "    \n",
    "    scaled_factor = data_scale * factor + prior_scale * torch.eye(factor.shape[0]).type(factor.type())\n",
    "    inv_factor = torch.inverse(scaled_factor)\n",
    "    chol_inv_factor = torch.cholesky(inv_factor, upper=upper)\n",
    "    return chol_inv_factor\n",
    "\n",
    "def sample_K_laplace_MN(MAP, upper_Qinv, lower_HHinv):\n",
    "    # H = Qi (kron) HHi\n",
    "    # sample isotropic unit variance mtrix normal\n",
    "    Z = MAP.data.new(MAP.size()).normal_(mean=0, std=1)\n",
    "    # AAT = HHi\n",
    "    # A = torch.cholesky(HHinv, upper=False)\n",
    "    # BTB = Qi\n",
    "    # B = torch.cholesky(Qinv, upper=True)\n",
    "    all_mtx_sample = MAP + torch.matmul(torch.matmul(lower_HHinv, Z), upper_Qinv)\n",
    "    \n",
    "    weight_mtx_sample = all_mtx_sample[:, :-1]\n",
    "    bias_mtx_sample = all_mtx_sample[:, -1]\n",
    "    \n",
    "    return weight_mtx_sample, bias_mtx_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Network wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_nb_parameters(self):\n",
    "        return np.sum(p.numel() for p in self.model.parameters())\n",
    "\n",
    "    def set_mode_train(self, train=True):\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    def update_lr(self, epoch, gamma=0.99):\n",
    "        self.epoch += 1\n",
    "        if self.schedule is not None:\n",
    "            if len(self.schedule) == 0 or epoch in self.schedule:\n",
    "                self.lr *= gamma\n",
    "                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'lr': self.lr,\n",
    "            'model': self.model,\n",
    "            'optimizer': self.optimizer}, filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename)\n",
    "        self.epoch = state_dict['epoch']\n",
    "        self.lr = state_dict['lr']\n",
    "        self.model = state_dict['model']\n",
    "        self.optimizer = state_dict['optimizer']\n",
    "        print('restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n",
    "        return self.epoch\n",
    "    \n",
    "class KBayes_Net(BaseNet):\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, lr=1e-3, channels_in=1, side_in=28, cuda=False, classes=10, batch_size=128, prior_sig=0):\n",
    "        super(KBayes_Net, self).__init__()\n",
    "        print('Net created.')\n",
    "        self.lr = lr\n",
    "        self.schedule = None  # [] #[50,200,400,600]\n",
    "        self.cuda = cuda\n",
    "        self.channels_in = channels_in\n",
    "        self.prior_sig = prior_sig\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "        self.side_in = side_in\n",
    "        self.create_net()\n",
    "        self.create_opt()\n",
    "        self.epoch = 0\n",
    "        self.test = False\n",
    "        \n",
    "    def create_net(self):\n",
    "        torch.manual_seed(42)\n",
    "        if self.cuda:\n",
    "            torch.cuda.manual_seed(42)\n",
    "        self.model = Net(input_dim=self.channels_in*self.side_in*self.side_in, output_dim=self.classes)\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "        print('Total params: %.2fK' % (self.get_nb_parameters() / 1000.0))\n",
    "    \n",
    "    def create_opt(self):\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.5, weight_decay=1/self.prior_sig**2)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')           \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # out: (batch_size, out_channels, out_caps_dims)\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "        return loss.data, err\n",
    "    \n",
    "    def eval(self, x, y, train=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        out = self.model(x)\n",
    "        loss = F.cross_entropy(out, y, reduction='sum')\n",
    "        probs = F.softmax(out, dim=1).data.cpu()\n",
    "        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "        return loss.data, err, probs\n",
    "    \n",
    "    def get_K_laplace_params(self, trainloader):\n",
    "        self.model.eval()\n",
    "        it_counter = 0\n",
    "        cum_HH1 = self.model.fc1.weight.data.new(self.model.n_hid, self.model.n_hid).fill_(0)\n",
    "        cum_HH2 = self.model.fc1.weight.data.new(self.model.n_hid, self.model.n_hid).fill_(0)\n",
    "        cum_HH3 = self.model.fc1.weight.data.new(self.model.output_dim, self.model.output_dim).fill_(0)\n",
    "\n",
    "        cum_Q1 = self.model.fc1.weight.data.new(self.model.input_dim+1, self.model.input_dim+1).fill_(0)\n",
    "        cum_Q2 = self.model.fc1.weight.data.new(self.model.n_hid+1, self.model.n_hid+1).fill_(0)\n",
    "        cum_Q3 = self.model.fc1.weight.data.new(self.model.n_hid+1, self.model.n_hid+1).fill_(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        for x, y in trainloader:\n",
    "\n",
    "            x, y = to_variable(var=(x, y.long()), cuda=use_cuda)\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(x)\n",
    "            out_act = F.softmax(out, dim=1)\n",
    "            loss = F.cross_entropy(out, y, reduction='sum')\n",
    "            loss.backward()\n",
    "            \n",
    "            HH3 = softmax_CE_preact_hessian(out_act.data)\n",
    "            cum_HH3 += HH3.sum(dim=0)\n",
    "        \n",
    "            Q3 = torch.bmm(self.model.a2.data.unsqueeze(2), self.model.a2.data.unsqueeze(1))\n",
    "            cum_Q3 += Q3.sum(dim=0)\n",
    "            \n",
    "            HH2 = layer_act_hessian_recurse(prev_hessian=HH3, prev_weights=self.model.fc3.weight.data,\n",
    "                                        layer_pre_acts=self.model.h2.data)\n",
    "            cum_HH2 += HH2.sum(dim=0)\n",
    "            Q2 = torch.bmm(self.model.a1.data.unsqueeze(2), self.model.a1.data.unsqueeze(1))\n",
    "            cum_Q2 += Q2.sum(dim=0)\n",
    "            HH1 = layer_act_hessian_recurse(prev_hessian=HH2, prev_weights=self.model.fc2.weight.data,\n",
    "                                   layer_pre_acts=self.model.h1.data)\n",
    "            cum_HH1 += HH1.sum(dim=0)\n",
    "            Q1 = torch.bmm(self.model.a0.data.unsqueeze(2), self.model.a0.data.unsqueeze(1))\n",
    "            cum_Q1 += Q1.sum(dim=0)\n",
    "            \n",
    "            it_counter += x.shape[0]\n",
    "            print(it_counter)\n",
    "\n",
    "        EHH3 = cum_HH3 / it_counter\n",
    "        EHH2 = cum_HH2 / it_counter\n",
    "        EHH1 = cum_HH1 / it_counter\n",
    "\n",
    "        EQ3 = cum_Q3 / it_counter\n",
    "        EQ2 = cum_Q2 / it_counter\n",
    "        EQ1 = cum_Q1 / it_counter\n",
    "\n",
    "        MAP3 = torch.cat((self.model.fc3.weight.data, self.model.fc3.bias.data.unsqueeze(1)), dim=1)\n",
    "        MAP2 = torch.cat((self.model.fc2.weight.data, self.model.fc2.bias.data.unsqueeze(1)), dim=1)\n",
    "        MAP1 = torch.cat((self.model.fc1.weight.data, self.model.fc1.bias.data.unsqueeze(1)), dim=1)\n",
    "        \n",
    "        return EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3\n",
    "    \n",
    "    def sample_eval(self, x, y, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3, logits=False):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "\n",
    "        out = self.model.sample_predict(x, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3)\n",
    "        \n",
    "        if logits:\n",
    "            mean_out = out.mean(dim=0, keepdim=False)\n",
    "            loss = F.cross_entropy(mean_out, y, reduction='sum')\n",
    "            probs = F.softmax(mean_out, dim=1).data.cpu()\n",
    "            \n",
    "        else:\n",
    "            mean_out =  F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n",
    "            probs = mean_out.data.cpu()\n",
    "            \n",
    "            log_mean_probs_out = torch.log(mean_out)\n",
    "            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n",
    "\n",
    "        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n",
    "        err = pred.ne(y.data).sum()\n",
    "\n",
    "        return loss.data, err, probs\n",
    "    \n",
    "    def all_sample_eval(self, x, y, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3):\n",
    "        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n",
    "        out = self.model.sample_predict(x, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3)       \n",
    "        prob_out =  F.softmax(out, dim=2)\n",
    "        prob_out = prob_out.data\n",
    "        return prob_out\n",
    "    \n",
    "    def get_weight_samples(self, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3):\n",
    "        weight_vec = []      \n",
    "        for i in range(Nsamples):         \n",
    "            w1, b1 = sample_K_laplace_MN(MAP1, scale_inv_EQ1, scale_inv_EHH1)\n",
    "            w2, b2 = sample_K_laplace_MN(MAP2, scale_inv_EQ2, scale_inv_EHH2)\n",
    "            w3, b3 = sample_K_laplace_MN(MAP3, scale_inv_EQ3, scale_inv_EHH3)\n",
    "        \n",
    "            for weight in w1.cpu().numpy().flatten():\n",
    "                weight_vec.append(weight)\n",
    "            for weight in w2.cpu().numpy().flatten():\n",
    "                weight_vec.append(weight)\n",
    "            for weight in w3.cpu().numpy().flatten():\n",
    "                weight_vec.append(weight)\n",
    "            \n",
    "        return np.array(weight_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Load and normalize MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:7\" if use_cuda else \"cpu\")\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "torchvision.datasets.MNIST.resources = [\n",
    "   ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "   for url, md5 in datasets.MNIST.resources\n",
    "]\n",
    "dataset = datasets.MNIST(\n",
    "   \"./data\", train=True, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Split in train, validate and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset, testset = torch.utils.data.random_split(dataset,[5000,1000,54000])\n",
    "#Dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8,shuffle=True, num_workers=3)\n",
    "valloader = DataLoader(valset, batch_size=1)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net created.\n",
      "Total params: 3.00K\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "prior_sig = 10000\n",
    "batch_size = 100\n",
    "net = KBayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size, prior_sig=prior_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 10 \n",
    "nb_its_dev = 1\n",
    "\n",
    "pred_cost_train = np.zeros(nb_epochs)\n",
    "err_train = np.zeros(nb_epochs)\n",
    "\n",
    "cost_dev = np.zeros(nb_epochs)\n",
    "err_dev = np.zeros(nb_epochs)\n",
    "best_err = np.inf\n",
    "\n",
    "tic0 = time.time()\n",
    "epoch = 0\n",
    "for i in range(epoch, nb_epochs):    \n",
    "    net.set_mode_train(True)\n",
    "    tic = time.time()\n",
    "    nb_samples = 0\n",
    "    for x, y in trainloader:\n",
    "        cost_pred, err = net.fit(x, y)\n",
    "\n",
    "        err_train[i] += err\n",
    "        pred_cost_train[i] += cost_pred\n",
    "        nb_samples += len(x)\n",
    "\n",
    "    pred_cost_train[i] /= nb_samples\n",
    "    err_train[i] /= nb_samples\n",
    "\n",
    "    toc = time.time()\n",
    "    net.epoch = i\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr_pred = %f, err = %f, \" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end=\"\")\n",
    "    print(\"time: %f seconds\\n\"% (toc - tic))\n",
    "    # ---- dev\n",
    "    if i % nb_its_dev == 0:\n",
    "        net.set_mode_train(False)\n",
    "        nb_samples = 0\n",
    "        for j, (x, y) in enumerate(valloader):\n",
    "\n",
    "            cost, err, probs = net.eval(x, y)\n",
    "\n",
    "            cost_dev[i] += cost\n",
    "            err_dev[i] += err\n",
    "            nb_samples += len(x)\n",
    "\n",
    "        cost_dev[i] /= nb_samples\n",
    "        err_dev[i] /= nb_samples\n",
    "        print('Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "        if err_dev[i] < best_err:\n",
    "            best_err = err_dev[i]\n",
    "            net.save(models_dir+'/theta_best.dat')\n",
    "\n",
    "toc0 = time.time()\n",
    "runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n",
    "print('average time: %f seconds\\n' % runtime_per_it)\n",
    "# results\n",
    "print('\\nRESULTS:')\n",
    "nb_parameters = net.get_nb_parameters()\n",
    "best_cost_dev = np.min(cost_dev)\n",
    "best_cost_train = np.min(pred_cost_train)\n",
    "err_dev_min = err_dev[::nb_its_dev].min()\n",
    "\n",
    "print('  cost_dev: %f (cost_train %f)' % (best_cost_dev, best_cost_train))\n",
    "print('  err_dev: %f' % (err_dev_min))\n",
    "print('  nb_parameters: %d (%s)' % (nb_parameters, humansize(nb_parameters)))\n",
    "print('  time_per_it: %fs\\n' % (runtime_per_it))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Kron hessian approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, pin_memory=True, num_workers=3)\n",
    "EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3 = net.get_K_laplace_params(trainloader)\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "h_params = [EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3]\n",
    "save_object(h_params, '/block_hessian_params.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up net and hessian params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transform_train)\n",
    "valset = datasets.MNIST(root='../data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "if use_cuda:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=3)\n",
    "\n",
    "else:\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n",
    "                                              num_workers=3)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=3)\n",
    "\n",
    "lr = 1e-3\n",
    "prior_sig = 2\n",
    "net = KBayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size, prior_sig=prior_sig)\n",
    "net.load(models_dir+'/theta_best.dat')\n",
    "\n",
    "with open(models_dir+'/block_hessian_params.pkl', 'rb') as input:\n",
    "    [EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3] = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do scaling and get inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scale = np.sqrt(60000)\n",
    "\n",
    "prior_sig = 0.15\n",
    "\n",
    "prior_prec = 1/prior_sig**2\n",
    "prior_scale = np.sqrt(prior_prec)\n",
    "\n",
    "# upper_Qinv, lower_HHinv\n",
    "\n",
    "scale_inv_EQ1 = chol_scale_invert_kron_factor(EQ1, prior_scale, data_scale, upper=True)\n",
    "scale_inv_EHH1 = chol_scale_invert_kron_factor(EHH1, prior_scale, data_scale, upper=False)\n",
    "\n",
    "scale_inv_EQ2 = chol_scale_invert_kron_factor(EQ2, prior_scale, data_scale, upper=True)\n",
    "scale_inv_EHH2 = chol_scale_invert_kron_factor(EHH2, prior_scale, data_scale, upper=False)\n",
    "\n",
    "scale_inv_EQ3 = chol_scale_invert_kron_factor(EQ3, prior_scale, data_scale, upper=True)\n",
    "scale_inv_EHH3 = chol_scale_invert_kron_factor(EHH3, prior_scale, data_scale, upper=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "if use_cuda:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "else:\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n",
    "                                            num_workers=4)\n",
    "\n",
    "nb_samples = 0    \n",
    "\n",
    "test_cost = 0  # Note that these are per sample\n",
    "test_err = 0\n",
    "test_predictions = np.zeros((10000, 10))\n",
    "\n",
    "net.set_mode_train(False)\n",
    "\n",
    "for j, (x, y) in enumerate(valloader):\n",
    "    cost, err, probs = net.eval(x, y) # \n",
    "    \n",
    "\n",
    "    test_cost += cost\n",
    "    test_err += err.cpu().numpy()\n",
    "    test_predictions[nb_samples:nb_samples+len(x), :] = probs.numpy()\n",
    "    nb_samples += len(x)\n",
    "\n",
    "test_err /= nb_samples\n",
    "print('Loglike = %5.6f, err = %1.6f\\n' % (-test_cost, test_err))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
